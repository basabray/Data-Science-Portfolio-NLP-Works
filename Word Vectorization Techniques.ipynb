{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorization different techniques:\n",
    "### `1.Bag of words`\n",
    "### `2.n-grams`\n",
    "### `3.TF-IDF`\n",
    "\n",
    "### Bag of words : bag of words are nothing but the frequency of words in a sentence/text/doc. The frequencies are represented in the form of a matrix. In case of NLP, we need to convert the text data into form of vectors so that we can feed those as input data to the NLP Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features are: ['examples', 'is', 'of', 'one', 'the', 'this']\n",
      "The word vectors are: [[1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "## Bag Of Words :example1 (single sentence)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "string = [\"This is one of the examples one !\"]\n",
    "vect1 = CountVectorizer()\n",
    "vect1.fit_transform(string)\n",
    "print(\"Features are:\", vect1.get_feature_names())              #get the features/words\n",
    "\n",
    "\n",
    "#Now store the vectors created from the words frequencies:\n",
    "x = vect1.fit_transform(string).toarray()                      #get the vector of the wordfrequencies  \n",
    "print('The word vectors are:', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- This corpus has only 1 sentence in it. Total 6 words are present in the sentence \n",
    "- The word 'one' has appeared two times in the text, so the value = 2. the rest has appeared only once in the text/string/sentence, hence the vlaue = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus is: ['word using analysis ted talk using word analytics learning key', 'using learning key grabbed land culture history tried enforce way life', 'respect freedom others', 'first vision freedom', 'anobody vision freedom']\n",
      "\n",
      "[[1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 2 0 0 2]\n",
      " [0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "## Bag Of Words :example2 (multile sentences)\n",
    "text = 'words that using is and analysis on TED talks and on using words analytics and learning key. Using learning is the key here.We have not grabbed their land, their culture, their history and tried to enforce our way of life on them. Why, because we respect the freedom of others. That is why my first vision is that of freedom. Anobody has the vision of freedom.'\n",
    "\n",
    "# Cleaning the text data\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords            #removed stop words\n",
    "from nltk.stem import WordNetLemmatizer      #converting the words into their base forms using lemmatization\n",
    "\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "print('The corpus is:', corpus)\n",
    "print()\n",
    "\n",
    "vect2 = CountVectorizer()   \n",
    "freq_matrix = vect2.fit_transform(corpus).toarray()\n",
    "\n",
    "print(freq_matrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vector of Sentence 1: [1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 2 0 0 2]\n",
    "- Vector of Sentence 2: [0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0] and so on   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- In this corpus there are total 5 sentences present\n",
    "- For each sentence, word frequencies are shown in the form of a matrix\n",
    "- We can see that there are total 2 highest appeared words (with frequency = 2)\n",
    "- And the highest occured words in this corpus belongs to the first sentence and second sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawbacks of using a Bag-of-Words (BoW) Model: In the above example, we have vectors of length 22. However, we will face issues when we come across new sentences\n",
    "- If the new sentences contain new words, then our vocabulary size would increase and thereby, the length of the vectors would increase too.\n",
    "- Additionally, the vectors would also contain many 0s, thereby resulting in a sparse matrix (which is what we would like to avoid)\n",
    "- We are retaining no information on the grammar of the sentences nor on the ordering of the words in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams: An N-gram is a sequence of N number of tokens (or words). We can think n-grams as extension of the BOG Model with some modification to it. In BOG we considered each word as a single token. In n-gram we have the freedom to choose n number of words together as a token\n",
    "- n= 1, we call it a “unigram”\n",
    "- n=2, it is called a “bigram”\n",
    "- n=3, it is called a “trigram”\n",
    "- We can clearly see that BOW model is nothing but n-gram model when n=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram  : ['example', 'is', 'the', 'this', 'two']\n",
      "2-gram  : ['example two', 'is the', 'the example', 'this is']\n",
      "3-gram  : ['is the example', 'the example two', 'this is the']\n",
      "4-gram  : ['is the example two', 'this is the example']\n"
     ]
    }
   ],
   "source": [
    "#n-grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "string = [\"This is the example two!\"]\n",
    "\n",
    "#unigram\n",
    "vect1 = CountVectorizer(ngram_range=(1,1))\n",
    "vect1.fit_transform(string)\n",
    "\n",
    "#bigram\n",
    "vect2 = CountVectorizer(ngram_range=(2,2))\n",
    "vect2.fit_transform(string)\n",
    "\n",
    "#trigram\n",
    "vect3 = CountVectorizer(ngram_range=(3,3))\n",
    "vect3.fit_transform(string)\n",
    "\n",
    "#4gram\n",
    "vect4 = CountVectorizer(ngram_range=(4,4))\n",
    "vect4.fit_transform(string)\n",
    "\n",
    "\n",
    "print(\"1-gram  :\",vect1.get_feature_names())\n",
    "print(\"2-gram  :\",vect2.get_feature_names())\n",
    "print(\"3-gram  :\",vect3.get_feature_names())\n",
    "print(\"4-gram  :\",vect4.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect4.fit_transform(string).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of N-gram approach to Language Modeling : N-gram based language models do have a few drawbacks:\n",
    "- The higher the N, the better is the model usually. But this leads to lots of computation overhead that requires large computation power\n",
    "- N-grams are a sparse representation of language. This is because we build the model based on the probability of words co-occurring. It will give zero probability to all the words that are not present in the training corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF: \n",
    "\n",
    "###### 1.Term Frequency\n",
    "- It is simply the frequency in which a word appears in a document in comparison to the total number words in the document. Mathematically given as: \n",
    "- TF = (Number of times a word appears in the document) / (Total number of words in the document)\n",
    "\n",
    "###### 2.Inverse Document Frequency\n",
    "- Term frequency has a disadvantage that it tends to give higher weights to words with higher frequency. In such cases words like ‘a’, ‘the’, ‘in’, ’of’ etc. appears more in the documents than other regular words. Thus, more important words are wrongly given lower weights as their frequency is less. To tackle this problem IDF was introduced. IDF decreases the weights of such high frequency terms and increases the weight of terms with rare occurrence. Mathematically it is given as:\n",
    "- IDF = log [(Number of documents)/(Number of documents the word appears in)]\n",
    "- note: [log has base 2]\n",
    "\n",
    "##### Finally, Tf-Idf Score = TF * IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus is: ['word using analysis word ted talk using word analytics learning key', 'using learning key grabbed land culture history tried enforce way life', 'respect freedom others', 'first vision freedom', 'anobody vision freedom']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>analytics</th>\n",
       "      <th>anobody</th>\n",
       "      <th>culture</th>\n",
       "      <th>enforce</th>\n",
       "      <th>first</th>\n",
       "      <th>freedom</th>\n",
       "      <th>grabbed</th>\n",
       "      <th>history</th>\n",
       "      <th>key</th>\n",
       "      <th>...</th>\n",
       "      <th>life</th>\n",
       "      <th>others</th>\n",
       "      <th>respect</th>\n",
       "      <th>talk</th>\n",
       "      <th>ted</th>\n",
       "      <th>tried</th>\n",
       "      <th>using</th>\n",
       "      <th>vision</th>\n",
       "      <th>way</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.243213</td>\n",
       "      <td>0.243213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.243213</td>\n",
       "      <td>0.243213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.729638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316978</td>\n",
       "      <td>0.316978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316978</td>\n",
       "      <td>0.316978</td>\n",
       "      <td>0.255735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316978</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316978</td>\n",
       "      <td>0.255735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316978</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63907</td>\n",
       "      <td>0.63907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690159</td>\n",
       "      <td>0.462208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.462208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  analytics   anobody   culture   enforce     first   freedom  \\\n",
       "0  0.243213   0.243213  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000   0.000000  0.000000  0.316978  0.316978  0.000000  0.000000   \n",
       "2  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.427993   \n",
       "3  0.000000   0.000000  0.000000  0.000000  0.000000  0.690159  0.462208   \n",
       "4  0.000000   0.000000  0.690159  0.000000  0.000000  0.000000  0.462208   \n",
       "\n",
       "    grabbed   history       key  ...      life   others  respect      talk  \\\n",
       "0  0.000000  0.000000  0.196222  ...  0.000000  0.00000  0.00000  0.243213   \n",
       "1  0.316978  0.316978  0.255735  ...  0.316978  0.00000  0.00000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  ...  0.000000  0.63907  0.63907  0.000000   \n",
       "3  0.000000  0.000000  0.000000  ...  0.000000  0.00000  0.00000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  ...  0.000000  0.00000  0.00000  0.000000   \n",
       "\n",
       "        ted     tried     using    vision       way      word  \n",
       "0  0.243213  0.000000  0.392445  0.000000  0.000000  0.729638  \n",
       "1  0.000000  0.316978  0.255735  0.000000  0.316978  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.556816  0.000000  0.000000  \n",
       "4  0.000000  0.000000  0.000000  0.556816  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'words that using is and analysis on words TED talks and on using words analytics and learning key. Using learning is the key here.We have not grabbed their land, their culture, their history and tried to enforce our way of life on them. Why, because we respect the freedom of others. That is why my first vision is that of freedom. Anobody has the vision of freedom.'\n",
    "\n",
    "# Cleaning the text data\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords            #removed stop words\n",
    "from nltk.stem import WordNetLemmatizer      #converting the words into their base forms using lemmatization\n",
    "\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "print('The corpus is:', corpus)\n",
    "print()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "x = tfidf.fit_transform(corpus)\n",
    "df = pd.DataFrame(x.todense(),columns=tfidf.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
